# CSE310_Software_Portfolio

# Overview
Our goal is to learn how to use ROS to interact with the physical world and 
people. We want the arm to reach for a point designated by the user by touching
the point. It will also respond to voice commands (turn left or right, up or 
down, etc.). We will be learning Open CV and speech recognition to utilize the
commands mentioned earlier.

# Development Environment
The Raspberry Pi will be the brain of the project, running Robot Operating 
System Noetic. Python will be the main language in use. OpenCV will be used 
for object recognition and location. PocketSphinx will be used for voice 
commands (i.e. 'turn left', 'look right'...). eSpeak will be used for voice 
responses and status updates. 

# Useful Websites
[Stack Overflow](https://stackoverflow.com/)  
[ROS Wiki](http://wiki.ros.org/Documentation)  
[YouTube](https://youtube.com)

# TODO
* N/A

# Story
This is a project to help us and other students to learn OpenCV and ROS. 
This can be extended
to any other robotics systems from a fruit picker to an ankle biter 
to a hunter drone.

# Requirements
Flite  
PocketSphinx  
OpenCv  
ROS  
CORAL - Run "git clone https://github.com/google-coral/examples-camera.git --depth 1" in terminal  

## Authors:
### Jeff Marsh and Adam Amott
