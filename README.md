# CSE310_Software_Portfolio

# Overview
Our goal is to learn how to use ROS to interact with the physical world and people. We want the arm to reach for a point designated by the user by touching the point. It will also respond to voice commands (turn left or right, up or down, etc.). We will be learning Open CV and speech recognition to utilize the commands mentioned earlier.
<!--{Important!  Do not say in this section that this is college assignment.  Talk about what you are trying to accomplish as a software engineer to further your learning.} -->

# Development Environment
The Raspberry Pi will be the brain of the project, running Robot Operating System Noetic. Python will be the main language in use. OpenCV will be used for object recognition and location. PocketSphinx will be used for voice commands (i.e. 'turn left', 'look right'...). eSpeak will be used for voice responses and status updates. 

# Useful Websites
*Coming Soon*

# Future Work
* Build Robot Operating System Environment
* Servos
* PocketSphinx and eSpeak
* Finish move_arm
* Voice input

# Story
This is a project to help us and other students to learn OpenCV and ROS. This can be extended
to any other robotics systems from a fruit picker to an ankle biter/pooper scooper.

# Requirements
flite
opencv
ros
coral - Run "git clone https://github.com/google-coral/examples-camera.git --depth 1" in terminal

## Authors:
### Jeff Marsh and Adam Amott
